{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12187599,"sourceType":"datasetVersion","datasetId":7676600},{"sourceId":12188128,"sourceType":"datasetVersion","datasetId":7676936},{"sourceId":12188150,"sourceType":"datasetVersion","datasetId":7676952},{"sourceId":12188197,"sourceType":"datasetVersion","datasetId":7676979},{"sourceId":12188279,"sourceType":"datasetVersion","datasetId":7677030},{"sourceId":12188645,"sourceType":"datasetVersion","datasetId":7677135},{"sourceId":12205069,"sourceType":"datasetVersion","datasetId":7677053}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport geopandas as gpd\nimport seaborn as sns\nimport random\nimport torch\nimport torch.nn as nn\nfrom typing import List, Tuple\nimport shay\nimport shay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:16:57.764508Z","iopub.execute_input":"2025-06-18T15:16:57.764848Z","iopub.status.idle":"2025-06-18T15:16:57.771628Z","shell.execute_reply.started":"2025-06-18T15:16:57.764826Z","shell.execute_reply":"2025-06-18T15:16:57.770614Z"}},"outputs":[],"execution_count":168},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/adsb-traces-v5/traces_segmented.csv')\ndf = df[['segment_id', 'timestamp', 'longitude', 'latitude', 'altitude']]\ndf = df.sort_values(['segment_id', 'timestamp'])\ndf = df.drop_duplicates(subset=['segment_id', 'timestamp', 'longitude', 'latitude', 'altitude'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:04.131421Z","iopub.execute_input":"2025-06-18T14:00:04.131986Z","iopub.status.idle":"2025-06-18T14:00:05.078884Z","shell.execute_reply.started":"2025-06-18T14:00:04.131944Z","shell.execute_reply":"2025-06-18T14:00:05.077964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:06.259089Z","iopub.execute_input":"2025-06-18T14:00:06.259497Z","iopub.status.idle":"2025-06-18T14:00:06.275418Z","shell.execute_reply.started":"2025-06-18T14:00:06.259467Z","shell.execute_reply":"2025-06-18T14:00:06.274438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['altitude'] = pd.to_numeric(df['altitude'], errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:06.531822Z","iopub.execute_input":"2025-06-18T14:00:06.533097Z","iopub.status.idle":"2025-06-18T14:00:06.724714Z","shell.execute_reply.started":"2025-06-18T14:00:06.533050Z","shell.execute_reply":"2025-06-18T14:00:06.723802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:06.879550Z","iopub.execute_input":"2025-06-18T14:00:06.879867Z","iopub.status.idle":"2025-06-18T14:00:06.918768Z","shell.execute_reply.started":"2025-06-18T14:00:06.879841Z","shell.execute_reply":"2025-06-18T14:00:06.917577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"world = gpd.read_file(\"/kaggle/input/world-map/ne_110m_admin_0_countries.shx\")\n\nisrael = world[world['ADMIN'] == 'Israel']\nisrael_neigbors = world[world['ADMIN'].isin(['Israel', 'Jordan', 'Egypt', 'Syria', 'Lebanon'])]\n\n# df = pd.read_csv('adsb_traces_israel_stream.csv')\n\ngdf_points = gpd.GeoDataFrame(\n    df,\n    geometry=gpd.points_from_xy(df['longitude'], df['latitude']),\n    crs=\"EPSG:4326\"\n)\n\nfig, ax = plt.subplots(figsize=(20, 20))\n\nisrael_neigbors.boundary.plot(ax=ax, color='black', linewidth=1.2)\n\ngdf_points.plot(ax=ax, markersize=1, alpha=0.2, color='blue')\n\nax.set_title('Aircraft Traces over Israel', fontsize=14)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n# ax.set_xlim([29, 37])\n# ax.set_ylim([27, 38])\n\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:07.119953Z","iopub.execute_input":"2025-06-18T14:00:07.120270Z","iopub.status.idle":"2025-06-18T14:00:19.376797Z","shell.execute_reply.started":"2025-06-18T14:00:07.120245Z","shell.execute_reply":"2025-06-18T14:00:19.375571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[(df['longitude'].between(29, 37)) & df['latitude'].between(27, 38)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.378605Z","iopub.execute_input":"2025-06-18T14:00:19.378985Z","iopub.status.idle":"2025-06-18T14:00:19.401390Z","shell.execute_reply.started":"2025-06-18T14:00:19.378955Z","shell.execute_reply":"2025-06-18T14:00:19.399645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Filter Tracks","metadata":{}},{"cell_type":"code","source":"tracks_count = df.groupby('segment_id')['timestamp'].count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.402606Z","iopub.execute_input":"2025-06-18T14:00:19.402966Z","iopub.status.idle":"2025-06-18T14:00:19.455809Z","shell.execute_reply.started":"2025-06-18T14:00:19.402936Z","shell.execute_reply":"2025-06-18T14:00:19.454668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tracks_count.describe(percentiles=[0.2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.458673Z","iopub.execute_input":"2025-06-18T14:00:19.459121Z","iopub.status.idle":"2025-06-18T14:00:19.471479Z","shell.execute_reply.started":"2025-06-18T14:00:19.459085Z","shell.execute_reply":"2025-06-18T14:00:19.470305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"short_tracks = tracks_count[tracks_count < 100].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.472693Z","iopub.execute_input":"2025-06-18T14:00:19.472976Z","iopub.status.idle":"2025-06-18T14:00:19.487379Z","shell.execute_reply.started":"2025-06-18T14:00:19.472953Z","shell.execute_reply":"2025-06-18T14:00:19.486042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tracks_duration = df.sort_values(['segment_id', 'timestamp'])\\\n.groupby('segment_id')\\\n.apply(lambda x: x['timestamp'].max() - x['timestamp'].min())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.488863Z","iopub.execute_input":"2025-06-18T14:00:19.489211Z","iopub.status.idle":"2025-06-18T14:00:19.815122Z","shell.execute_reply.started":"2025-06-18T14:00:19.489180Z","shell.execute_reply":"2025-06-18T14:00:19.813835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tracks_duration.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.816128Z","iopub.execute_input":"2025-06-18T14:00:19.816465Z","iopub.status.idle":"2025-06-18T14:00:19.826298Z","shell.execute_reply.started":"2025-06-18T14:00:19.816432Z","shell.execute_reply":"2025-06-18T14:00:19.825381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"short_duration_tracks = tracks_duration[tracks_duration < 60 * 10].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.827188Z","iopub.execute_input":"2025-06-18T14:00:19.827473Z","iopub.status.idle":"2025-06-18T14:00:19.845065Z","shell.execute_reply.started":"2025-06-18T14:00:19.827450Z","shell.execute_reply":"2025-06-18T14:00:19.843954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[(~df['segment_id'].isin(short_tracks)) & (~df['segment_id'].isin(short_duration_tracks))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.846529Z","iopub.execute_input":"2025-06-18T14:00:19.846910Z","iopub.status.idle":"2025-06-18T14:00:19.892884Z","shell.execute_reply.started":"2025-06-18T14:00:19.846843Z","shell.execute_reply":"2025-06-18T14:00:19.891832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tracks_duration = df.sort_values(['segment_id', 'timestamp'])\\\n.groupby('segment_id')\\\n.apply(lambda x: x['timestamp'].max() - x['timestamp'].min())\ntracks_duration[tracks_duration < 60 * 10].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:19.896562Z","iopub.execute_input":"2025-06-18T14:00:19.896883Z","iopub.status.idle":"2025-06-18T14:00:20.124120Z","shell.execute_reply.started":"2025-06-18T14:00:19.896856Z","shell.execute_reply":"2025-06-18T14:00:20.123047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tracks_count = df.groupby('segment_id')['timestamp'].count()\ntracks_count[tracks_count < 50].index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:20.125225Z","iopub.execute_input":"2025-06-18T14:00:20.125532Z","iopub.status.idle":"2025-06-18T14:00:20.150738Z","shell.execute_reply.started":"2025-06-18T14:00:20.125507Z","shell.execute_reply":"2025-06-18T14:00:20.149671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### eda","metadata":{}},{"cell_type":"code","source":"df.groupby('segment_id')['timestamp'].count().sort_values().iloc[1100:1105]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:20.151742Z","iopub.execute_input":"2025-06-18T14:00:20.152036Z","iopub.status.idle":"2025-06-18T14:00:20.177067Z","shell.execute_reply.started":"2025-06-18T14:00:20.152006Z","shell.execute_reply":"2025-06-18T14:00:20.176193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['segment_id'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:20.178180Z","iopub.execute_input":"2025-06-18T14:00:20.178522Z","iopub.status.idle":"2025-06-18T14:00:20.207562Z","shell.execute_reply.started":"2025-06-18T14:00:20.178491Z","shell.execute_reply":"2025-06-18T14:00:20.206604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"temp_track = df[df['icao'] == '46b823']\ntemp_track.sort_values('timestamp')","metadata":{"execution":{"iopub.status.busy":"2025-06-16T20:43:02.941995Z","iopub.execute_input":"2025-06-16T20:43:02.942861Z","iopub.status.idle":"2025-06-16T20:43:02.972370Z","shell.execute_reply.started":"2025-06-16T20:43:02.942821Z","shell.execute_reply":"2025-06-16T20:43:02.971311Z"}}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 20))\n\nsns.scatterplot(ax=ax, data=df[df['segment_id'] == '471f69_2'], s=5, x='longitude', y='latitude', hue='timestamp')\nisrael.boundary.plot(ax=ax, color='black', linewidth=1.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:20.208770Z","iopub.execute_input":"2025-06-18T14:00:20.209138Z","iopub.status.idle":"2025-06-18T14:00:20.722586Z","shell.execute_reply.started":"2025-06-18T14:00:20.209105Z","shell.execute_reply":"2025-06-18T14:00:20.721560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"def process_flight_tracks(df):\n    processed_trajectories = {}\n\n    # Group by ICAO (aircraft ID)\n    for icao, group in df.groupby('segment_id'):\n        group = group.sort_values('timestamp').reset_index(drop=True)\n        \n        # Drop rows missing required columns\n        group = group.dropna(subset=['longitude', 'latitude', 'altitude', 'timestamp'])\n\n        # Skip if too short\n        if len(group) < 100:\n            continue\n\n        # Time difference\n        dt = group['timestamp'].diff().fillna(1)\n\n        # Compute component velocities\n        Vlon = group['longitude'].diff().fillna(0) / dt\n        Vlat = group['latitude'].diff().fillna(0) / dt\n        Valt = group['altitude'].diff().fillna(0) / dt\n\n        traj = np.stack([\n            group['longitude'].values,\n            group['latitude'].values,\n            group['altitude'].values,\n            Vlon.values,\n            Vlat.values,\n            Valt.values\n        ], axis=1)\n\n        processed_trajectories[icao] = traj\n\n    return processed_trajectories","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:20.723739Z","iopub.execute_input":"2025-06-18T14:00:20.724084Z","iopub.status.idle":"2025-06-18T14:00:20.732830Z","shell.execute_reply.started":"2025-06-18T14:00:20.724053Z","shell.execute_reply":"2025-06-18T14:00:20.731846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_trajectories = process_flight_tracks(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:20.733799Z","iopub.execute_input":"2025-06-18T14:00:20.734155Z","iopub.status.idle":"2025-06-18T14:00:23.812909Z","shell.execute_reply.started":"2025-06-18T14:00:20.734122Z","shell.execute_reply":"2025-06-18T14:00:23.811974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(processed_trajectories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:23.814053Z","iopub.execute_input":"2025-06-18T14:00:23.814740Z","iopub.status.idle":"2025-06-18T14:00:23.820621Z","shell.execute_reply.started":"2025-06-18T14:00:23.814713Z","shell.execute_reply":"2025-06-18T14:00:23.819725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def normalize_point(p):\n#     return np.array([p[0], p[1], p[2] / 1000])\n\ndef compute_angles(traj, k=1):\n    angles = []\n    for i in range(len(traj)):\n        if i - k < 0 or i + k >= len(traj):\n            angles.append(np.pi)  # padding with 0 if out of bounds\n            continue\n\n        # use altitude\n        # a = normalize_point(traj[i - k][:3])\n        # b = normalize_point(traj[i][:3])\n        # c = normalize_point(traj[i + k][:3])\n\n        a = traj[i - k][:2]\n        b = traj[i][:2]\n        c = traj[i + k][:2]\n\n        ba = a - b\n        bc = c - b\n        cos_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n        angles.append(angle)\n\n    return angles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:23.821672Z","iopub.execute_input":"2025-06-18T14:00:23.821918Z","iopub.status.idle":"2025-06-18T14:00:23.841708Z","shell.execute_reply.started":"2025-06-18T14:00:23.821898Z","shell.execute_reply":"2025-06-18T14:00:23.840857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def behavior_patching(traj, angle_thresh=np.pi/180, patch_size=32, total_patches=64):\n    angles = compute_angles(traj)\n    active_idxs = [i for i, a in enumerate(angles) if abs(a - np.pi) > angle_thresh]\n\n    # Cluster active points into patches\n    clusters = []\n    cluster = []\n    for idx in active_idxs:\n        if cluster and idx - cluster[-1] > 1:\n            clusters.append(cluster)\n            cluster = []\n        cluster.append(idx)\n    if cluster:\n        clusters.append(cluster)\n\n    behavior_patches = []\n    for clust in clusters:\n        center = clust[len(clust) // 2]\n        start = max(0, center - patch_size // 2)\n        end = min(len(traj), start + patch_size)\n        patch = traj[start:end]\n        if len(patch) == patch_size:\n            behavior_patches.append(patch)\n\n    # Select total_pattches overall\n    if len(behavior_patches) >= total_patches:\n        # Too many behavior patches — randomly keep only total_patches\n        behavior_patches = random.sample(behavior_patches, total_patches)\n        non_behavioral_patches = []\n    else:\n        # Fill in the rest with non-behavior patches\n        remaining_budget = total_patches - len(behavior_patches)\n\n        # Remaining patches from non-behavioral parts\n        non_behavioral_patches = []\n        remaining_budget = total_patches - len(behavior_patches)\n        available_range = len(traj) - patch_size\n    \n        # Compute starts to evenly spread remaining patches across the trajectory\n        if remaining_budget > 0 and available_range > 0:\n            starts = np.linspace(0, len(traj) - patch_size, remaining_budget, dtype=int)\n            for i in starts:\n                patch = traj[i:i + patch_size]\n                if len(patch) == patch_size:\n                    non_behavioral_patches.append(patch)\n    return (behavior_patches + non_behavioral_patches, list(range(len(behavior_patches)))), len(behavior_patches), len(non_behavioral_patches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:23.842821Z","iopub.execute_input":"2025-06-18T14:00:23.843095Z","iopub.status.idle":"2025-06-18T14:00:23.855768Z","shell.execute_reply.started":"2025-06-18T14:00:23.843071Z","shell.execute_reply":"2025-06-18T14:00:23.854751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patches = {}\nbehevior_count = []\nnon_behevior_count = []\nfor traj_id in processed_trajectories:\n    patches[traj_id], bc, nbc = behavior_patching(processed_trajectories[traj_id])\n    behevior_count.append(bc)\n    non_behevior_count.append(nbc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:23.856791Z","iopub.execute_input":"2025-06-18T14:00:23.857078Z","iopub.status.idle":"2025-06-18T14:00:28.038373Z","shell.execute_reply.started":"2025-06-18T14:00:23.857057Z","shell.execute_reply":"2025-06-18T14:00:28.037417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(10, 5), ncols=2)\naxes[0].set_title('behevior count')\nsns.histplot(behevior_count, ax=axes[0])\naxes[1].set_title('non behevior count')\nsns.histplot(non_behevior_count, ax=axes[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:28.039302Z","iopub.execute_input":"2025-06-18T14:00:28.039597Z","iopub.status.idle":"2025-06-18T14:00:28.559161Z","shell.execute_reply.started":"2025-06-18T14:00:28.039574Z","shell.execute_reply":"2025-06-18T14:00:28.558122Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Understading preprocess\nseems tracks are dense it turns","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=df[df['segment_id'] == '471f69_2'].iloc[:20], x='longitude', y='latitude', hue='altitude', s=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:28.560133Z","iopub.execute_input":"2025-06-18T14:00:28.560429Z","iopub.status.idle":"2025-06-18T14:00:28.918731Z","shell.execute_reply.started":"2025-06-18T14:00:28.560399Z","shell.execute_reply":"2025-06-18T14:00:28.917797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df[df['segment_id'] == '471f69_2'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:28.919904Z","iopub.execute_input":"2025-06-18T14:00:28.920242Z","iopub.status.idle":"2025-06-18T14:00:28.949721Z","shell.execute_reply.started":"2025-06-18T14:00:28.920211Z","shell.execute_reply":"2025-06-18T14:00:28.948638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_trajectories['471f69_2'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:28.950833Z","iopub.execute_input":"2025-06-18T14:00:28.951185Z","iopub.status.idle":"2025-06-18T14:00:28.969896Z","shell.execute_reply.started":"2025-06-18T14:00:28.951152Z","shell.execute_reply":"2025-06-18T14:00:28.968874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patches['471f69_2'], n_patch_behavior, n_patch_non_behavior = behavior_patching(processed_trajectories[traj_id])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:28.971013Z","iopub.execute_input":"2025-06-18T14:00:28.971347Z","iopub.status.idle":"2025-06-18T14:00:28.995257Z","shell.execute_reply.started":"2025-06-18T14:00:28.971317Z","shell.execute_reply":"2025-06-18T14:00:28.994240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"n_patch_behavior: {n_patch_behavior} | n_patch_non_behavior: {n_patch_non_behavior}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:28.996484Z","iopub.execute_input":"2025-06-18T14:00:28.997312Z","iopub.status.idle":"2025-06-18T14:00:29.003446Z","shell.execute_reply.started":"2025-06-18T14:00:28.997282Z","shell.execute_reply":"2025-06-18T14:00:29.001791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_trajectories['471f69_2']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.004640Z","iopub.execute_input":"2025-06-18T14:00:29.004974Z","iopub.status.idle":"2025-06-18T14:00:29.026832Z","shell.execute_reply.started":"2025-06-18T14:00:29.004941Z","shell.execute_reply":"2025-06-18T14:00:29.025675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(patches['471f69_2'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.031019Z","iopub.execute_input":"2025-06-18T14:00:29.031296Z","iopub.status.idle":"2025-06-18T14:00:29.047276Z","shell.execute_reply.started":"2025-06-18T14:00:29.031276Z","shell.execute_reply":"2025-06-18T14:00:29.046435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def behavior_patching_with_indices(traj, angle_thresh=np.pi/72, patch_size=16, total_patches=64):\n    angles = compute_angles(traj)\n    active_idxs = [i for i, a in enumerate(angles) if abs(a - np.pi) > angle_thresh]\n\n    # Cluster active indices\n    clusters = []\n    cluster = []\n    for idx in active_idxs:\n        if cluster and idx - cluster[-1] > 1:\n            clusters.append(cluster)\n            cluster = []\n        cluster.append(idx)\n    if cluster:\n        clusters.append(cluster)\n\n    behavior_patches = []\n    for clust in clusters:\n        center = clust[len(clust) // 2]\n        start = max(0, center - patch_size // 2)\n        end = start + patch_size\n        if end <= len(traj):\n            behavior_patches.append((traj[start:end], start))\n\n    if len(behavior_patches) >= total_patches:\n        behavior_patches = behavior_patches[:total_patches]\n        non_behavior_patches = []\n    else:\n        remaining_budget = total_patches - len(behavior_patches)\n        non_behavior_patches = []\n        starts = np.linspace(0, len(traj) - patch_size, remaining_budget, dtype=int)\n        for i in starts:\n            patch = traj[i:i + patch_size]\n            if len(patch) == patch_size:\n                non_behavior_patches.append((patch, i))\n\n    return behavior_patches + non_behavior_patches, len(behavior_patches), len(non_behavior_patches)  # list of (patch, start_index)\n\ndef plot_trajectory_with_patch_indices(traj, indexed_patches, behavior_count):\n    fig, axes = plt.subplots(figsize=(12, 18), nrows=3)\n    axes = axes.flatten()\n    axes[0].plot(traj[:, 0], traj[:, 1], color='gray', linewidth=2, label='Full Trajectory')\n    axes[1].plot(traj[:, 0], traj[:, 2], color='gray', linewidth=2, label='Full Trajectory')\n    axes[2].plot(traj[:, 1], traj[:, 2], color='gray', linewidth=2, label='Full Trajectory')\n\n    for i, (patch, start_idx) in enumerate(indexed_patches):\n        # color = 'red' if i < behavior_count else 'blue'\n        # label = 'Behavior Patch' if i == 0 and color == 'red' else 'Non-Behavior Patch' if i == behavior_count else ''\n        # plt.plot(traj[start_idx:start_idx+len(patch), 0],\n        #          traj[start_idx:start_idx+len(patch), 1],\n        #          color=color, linewidth=2, alpha=0.8, label=label)\n\n        if i >= behavior_count:\n            continue  # Skip non-behavior patches\n        \n        label = 'Behavior Patch' if i == 0 else None\n        axes[0].plot(traj[start_idx:start_idx+len(patch), 0],\n                 traj[start_idx:start_idx+len(patch), 1],\n                 color='red', linewidth=2, alpha=0.8, label=label)\n        \n        axes[1].plot(traj[start_idx:start_idx+len(patch), 0],\n                 traj[start_idx:start_idx+len(patch), 2],\n                 color='red', linewidth=2, alpha=0.8, label=label)\n        \n        axes[2].plot(traj[start_idx:start_idx+len(patch), 1],\n                 traj[start_idx:start_idx+len(patch), 2],\n                 color='red', linewidth=2, alpha=0.8, label=label)\n\n    plt.title(\"Trajectory Behavior and Non-Behavior Patches\")\n    axes[0].set_xlabel(\"Longitude\")\n    axes[0].set_ylabel(\"Latitude\")\n    axes[1].set_xlabel(\"Longitude\")\n    axes[1].set_ylabel(\"Altitude\")\n    axes[2].set_xlabel(\"Latitude\")\n    axes[2].set_ylabel(\"Altitude\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.048410Z","iopub.execute_input":"2025-06-18T14:00:29.048934Z","iopub.status.idle":"2025-06-18T14:00:29.072337Z","shell.execute_reply.started":"2025-06-18T14:00:29.048904Z","shell.execute_reply":"2025-06-18T14:00:29.071353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patches_track_sample, n_b, n_nb = behavior_patching_with_indices(processed_trajectories['471f69_2'])\nplot_trajectory_with_patch_indices(processed_trajectories['471f69_2'], patches_track_sample, n_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.073485Z","iopub.execute_input":"2025-06-18T14:00:29.073878Z","iopub.status.idle":"2025-06-18T14:00:29.733163Z","shell.execute_reply.started":"2025-06-18T14:00:29.073842Z","shell.execute_reply":"2025-06-18T14:00:29.732204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_nb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.734169Z","iopub.execute_input":"2025-06-18T14:00:29.734620Z","iopub.status.idle":"2025-06-18T14:00:29.741072Z","shell.execute_reply.started":"2025-06-18T14:00:29.734579Z","shell.execute_reply":"2025-06-18T14:00:29.740152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class PatchEncoder(nn.Module):\n    def __init__(self, input_dim=6, patch_size=32, dim=256, total_patches=64):\n        super().__init__()\n        self.projection = nn.Linear(input_dim * patch_size, dim)\n        self.pos_embedding = nn.Parameter(torch.randn(1, total_patches, dim))\n        self.layer_norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        # x shape: (batch_size, num_patches, patch_size, input_dim)\n        b, n, s, d = x.shape\n        x = x.view(b, n, s * d)\n        x = self.projection(x) + self.pos_embedding[:, :n, :]\n        x = self.layer_norm(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.742094Z","iopub.execute_input":"2025-06-18T14:00:29.742336Z","iopub.status.idle":"2025-06-18T14:00:29.760441Z","shell.execute_reply.started":"2025-06-18T14:00:29.742317Z","shell.execute_reply":"2025-06-18T14:00:29.759241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FLIGHT2VECModel(nn.Module):\n    \"\"\"Main FLIGHT2VEC model with behavior-adaptive patching and motion trend learning\"\"\"\n    \n    def __init__(self, \n                 input_dim=6, \n                 patch_size=32, \n                 dim=256, \n                 depth=3, \n                 heads=16, \n                 dropout=0.2,\n                 total_patches=64):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.patch_size = patch_size\n        self.dim = dim\n        \n        # Patch encoder\n        self.patch_encoder = PatchEncoder(\n            input_dim=input_dim, \n            patch_size=patch_size, \n            dim=dim, \n            total_patches=total_patches\n        )\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=dim, \n            nhead=heads, \n            dim_feedforward=dim * 4,\n            dropout=dropout,\n            activation='relu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        \n        # Reconstruction head for MSE loss\n        self.reconstruction_head = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim, patch_size * input_dim)\n        )\n        \n        # Motion direction prediction head (26 classes)\n        self.motion_head = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim, 26)\n        )\n        \n        # Global representation pooling\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        \n    def forward(self, patches, mask=None):\n        \"\"\"\n        Args:\n            patches: Input patches of shape (batch_size, num_patches, patch_size, input_dim)\n            mask: Optional mask for masked patches (batch_size, num_patches)\n        Returns:\n            Dictionary containing representations, reconstructions, and motion logits\n        \"\"\"\n        # Encode patches\n        x = self.patch_encoder(patches)  # (batch_size, num_patches, dim)\n        \n        # Apply transformer\n        transformer_out = self.transformer(x)  # (batch_size, num_patches, dim)\n        \n        # Reconstruction for masked patches\n        reconstructions = self.reconstruction_head(transformer_out)\n        reconstructions = reconstructions.view(-1, transformer_out.size(1), self.patch_size, self.input_dim)\n        \n        # Motion direction prediction\n        motion_logits = self.motion_head(transformer_out)  # (batch_size, num_patches, 26)\n        \n        # Global representation (average pooling over patches)\n        global_repr = transformer_out.mean(dim=1)  # (batch_size, dim)\n        \n        return {\n            'representations': transformer_out,\n            'global_representation': global_repr,\n            'reconstructions': reconstructions,\n            'motion_logits': motion_logits\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.761685Z","iopub.execute_input":"2025-06-18T14:00:29.762020Z","iopub.status.idle":"2025-06-18T14:00:29.788112Z","shell.execute_reply.started":"2025-06-18T14:00:29.761995Z","shell.execute_reply":"2025-06-18T14:00:29.786762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Flight2VecLoss(nn.Module):\n    def __init__(self, lambda_mse=1.0):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.ce = nn.CrossEntropyLoss()\n        self.lambda_mse = lambda_mse\n\n    def forward(self, preds, targets, motion_logits=None, motion_labels=None):\n        mse_loss = self.mse(preds, targets)\n        if motion_logits is not None and motion_labels is not None:\n            direction_loss = self.ce(motion_logits.view(-1, 26), motion_labels.view(-1))\n        else:\n            direction_loss = 0\n        return direction_loss + self.lambda_mse * mse_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.789269Z","iopub.execute_input":"2025-06-18T14:00:29.790290Z","iopub.status.idle":"2025-06-18T14:00:29.812073Z","shell.execute_reply.started":"2025-06-18T14:00:29.790250Z","shell.execute_reply":"2025-06-18T14:00:29.810982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patches_raw = np.array([np.array(p[0]) for p in patches.values()])\npatches_behavior_idxs = [p[1] for p in patches.values()]\npatches_raw.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:29.812846Z","iopub.execute_input":"2025-06-18T14:00:29.813232Z","iopub.status.idle":"2025-06-18T14:00:30.017838Z","shell.execute_reply.started":"2025-06-18T14:00:29.813199Z","shell.execute_reply":"2025-06-18T14:00:30.016888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Motion Trend Classification ","metadata":{}},{"cell_type":"code","source":"# class FLIGHT2VECModel(nn.Module):\n#     def __init__(self, dim=256, depth=3, heads=16):\n#         super().__init__()\n#         encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, batch_first=True)\n#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n#         self.encoder = PatchEncoder(dim=dim)\n#         self.mlp_head = nn.Sequential(nn.Linear(dim, dim), nn.ReLU(), nn.Linear(dim, 6))  # For reconstruction\n\n#     def forward(self, x):\n#         x = self.encoder(x)\n#         x = self.transformer(x)\n#         return x  # shape: (batch_size, num_patches, dim)\n\nclass FLIGHT2VECModel(nn.Module):\n    \"\"\"Main FLIGHT2VEC model with behavior-adaptive patching and motion trend learning\"\"\"\n    \n    def __init__(self, \n                 input_dim=6, \n                 patch_size=32, \n                 dim=256, \n                 depth=3, \n                 heads=16, \n                 dropout=0.2,\n                 total_patches=64):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.patch_size = patch_size\n        self.dim = dim\n        \n        # Patch encoder\n        self.patch_encoder = PatchEncoder(\n            input_dim=input_dim, \n            patch_size=patch_size, \n            dim=dim, \n            total_patches=total_patches\n        )\n        \n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=dim, \n            nhead=heads, \n            dim_feedforward=dim * 4,\n            dropout=dropout,\n            activation='relu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        \n        # Reconstruction head for MSE loss\n        self.reconstruction_head = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim, patch_size * input_dim)\n        )\n        \n        # Motion direction prediction head (26 classes)\n        self.motion_head = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim, 26)\n        )\n        \n        # Global representation pooling\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        \n    def forward(self, patches, mask=None):\n        \"\"\"\n        Args:\n            patches: Input patches of shape (batch_size, num_patches, patch_size, input_dim)\n            mask: Optional mask for masked patches (batch_size, num_patches)\n        Returns:\n            Dictionary containing representations, reconstructions, and motion logits\n        \"\"\"\n        # Encode patches\n        x = self.patch_encoder(patches)  # (batch_size, num_patches, dim)\n        \n        # Apply transformer\n        transformer_out = self.transformer(x)  # (batch_size, num_patches, dim)\n        \n        # Reconstruction for masked patches\n        reconstructions = self.reconstruction_head(transformer_out)\n        reconstructions = reconstructions.view(-1, transformer_out.size(1), self.patch_size, self.input_dim)\n        \n        # Motion direction prediction\n        motion_logits = self.motion_head(transformer_out)  # (batch_size, num_patches, 26)\n        \n        # Global representation (average pooling over patches)\n        global_repr = transformer_out.mean(dim=1)  # (batch_size, dim)\n        \n        return {\n            'representations': transformer_out,\n            'global_representation': global_repr,\n            'reconstructions': reconstructions,\n            'motion_logits': motion_logits\n        }\n\nclass MotionDirectionProcessor:\n    @staticmethod\n    def calculate_motion_directions(trajectories):\n        \"\"\"\n        Calculate motion direction classes for a list of trajectories with variable lengths\n        Args:\n            trajectories: list of np.ndarrays of shape (N, 6) where first 3 dims are [lon, lat, alt]\n        Returns:\n            list of direction label arrays\n        \"\"\"\n        direction_labels = []\n        for traj in trajectories:\n            coords = traj[:, :3]\n            direction_vectors = coords[1:] - coords[:-1]\n            # coord_scale = np.std(coords, axis=0)\n            # thresholds = np.maximum(coord_scale * 1e-3, 1e-6)\n            thresholds = [1e-6, 1e-6, 1e-6]\n\n            labels = []\n            for d_vec in direction_vectors:\n                d_lon = 1 if d_vec[0] > thresholds[0] else (-1 if d_vec[0] < -thresholds[0] else 0)\n                d_lat = 1 if d_vec[1] > thresholds[1] else (-1 if d_vec[1] < -thresholds[1] else 0)\n                d_alt = 1 if d_vec[2] > thresholds[2] else (-1 if d_vec[2] < -thresholds[2] else 0)\n\n                direction_tuple = (d_lon + 1, d_lat + 1, d_alt + 1)\n                direction_class = direction_tuple[0] * 9 + direction_tuple[1] * 3 + direction_tuple[2]\n\n                if direction_class == 13:\n                    direction_class = -1\n                elif direction_class > 13:\n                    direction_class -= 1\n\n                labels.append(direction_class)\n            direction_labels.append(np.array(labels))\n        return direction_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:30.019028Z","iopub.execute_input":"2025-06-18T14:00:30.019323Z","iopub.status.idle":"2025-06-18T14:00:30.035839Z","shell.execute_reply.started":"2025-06-18T14:00:30.019292Z","shell.execute_reply":"2025-06-18T14:00:30.034441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaskingStrategy:\n    \"\"\"Masking strategy for self-supervised learning\"\"\"\n    \n    def __init__(self, behavior_mask_prob=0.6, non_behavior_mask_prob=0.3):\n        self.behavior_mask_prob = behavior_mask_prob\n        self.non_behavior_mask_prob = non_behavior_mask_prob\n    \n    def create_mask(self, patches, behavior_indices=None):\n        \"\"\"\n        Create mask for patches based on behavior density\n        Args:\n            patches: Input patches (batch_size, num_patches, patch_size, input_dim)\n            behavior_indices: List of behavior patch indices for each batch\n        Returns:\n            mask: Boolean mask (batch_size, num_patches) - True for masked patches\n        \"\"\"\n        batch_size, num_patches = patches.shape[:2]\n        mask = torch.zeros(batch_size, num_patches, dtype=torch.bool)\n        \n        for batch_idx in range(batch_size):\n            if behavior_indices is not None and batch_idx < len(behavior_indices):\n                behavior_idx = behavior_indices[batch_idx]\n            else:\n                behavior_idx = []\n            \n            for patch_idx in range(num_patches):\n                if patch_idx in behavior_idx:\n                    # Mask behavior patches with higher probability\n                    if torch.rand(1).item() < self.behavior_mask_prob:\n                        mask[batch_idx, patch_idx] = True\n                else:\n                    # Mask non-behavior patches with lower probability\n                    if torch.rand(1).item() < self.non_behavior_mask_prob:\n                        mask[batch_idx, patch_idx] = True\n        \n        return mask\n\n# class MaskingStrategy:\n#     \"\"\"Improved masking strategy with more conservative probabilities\"\"\"\n    \n#     def __init__(self, behavior_mask_prob=0.4, non_behavior_mask_prob=0.2, min_unmasked=0.3):\n#         self.behavior_mask_prob = behavior_mask_prob\n#         self.non_behavior_mask_prob = non_behavior_mask_prob\n#         self.min_unmasked = min_unmasked  # Ensure at least 30% patches remain unmasked\n    \n#     def create_mask(self, patches, behavior_indices=None):\n#         \"\"\"\n#         Create mask for patches with guaranteed minimum unmasked patches\n#         \"\"\"\n#         batch_size, num_patches = patches.shape[:2]\n#         mask = torch.zeros(batch_size, num_patches, dtype=torch.bool)\n        \n#         for batch_idx in range(batch_size):\n#             behavior_idx = behavior_indices[batch_idx] if behavior_indices and batch_idx < len(behavior_indices) else []\n            \n#             # First pass: apply probabilistic masking\n#             temp_mask = torch.zeros(num_patches, dtype=torch.bool)\n#             for patch_idx in range(num_patches):\n#                 if patch_idx in behavior_idx:\n#                     if torch.rand(1).item() < self.behavior_mask_prob:\n#                         temp_mask[patch_idx] = True\n#                 else:\n#                     if torch.rand(1).item() < self.non_behavior_mask_prob:\n#                         temp_mask[patch_idx] = True\n            \n#             # Ensure minimum unmasked patches\n#             masked_count = temp_mask.sum().item()\n#             max_masked = int(num_patches * (1 - self.min_unmasked))\n            \n#             if masked_count > max_masked:\n#                 # Randomly unmask some patches\n#                 masked_indices = torch.where(temp_mask)[0]\n#                 unmask_count = masked_count - max_masked\n#                 unmask_indices = masked_indices[torch.randperm(len(masked_indices))[:unmask_count]]\n#                 temp_mask[unmask_indices] = False\n            \n#             mask[batch_idx] = temp_mask\n        \n#         return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:00:30.036986Z","iopub.execute_input":"2025-06-18T14:00:30.037451Z","iopub.status.idle":"2025-06-18T14:00:30.069393Z","shell.execute_reply.started":"2025-06-18T14:00:30.037415Z","shell.execute_reply":"2025-06-18T14:00:30.067836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"class FLIGHT2VECTrainer:\n    \"\"\"Training wrapper for FLIGHT2VEC\"\"\"\n    \n    def __init__(self, \n                 model, \n                 optimizer, \n                 criterion, \n                 device='cuda',\n                 masking_strategy=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.device = device\n        self.masking_strategy = masking_strategy or MaskingStrategy()\n        self.motion_processor = MotionDirectionProcessor()\n    \n    def train_step(self, patches, behavior_indices=None):\n        \"\"\"\n        Single training step\n        Args:\n            patches: numpy array of patches (batch_size, num_patches, patch_size, input_dim)\n            behavior_indices: List of behavior patch indices for each batch\n        \"\"\"\n        self.model.train()\n        \n        # Convert to tensor\n        patches_tensor = torch.FloatTensor(patches).to(self.device)\n        batch_size, num_patches, patch_size, input_dim = patches_tensor.shape\n        \n        # Calculate motion direction labels for each patch\n        motion_labels = []\n        for batch_idx in range(batch_size):\n            batch_labels = []\n            for patch_idx in range(num_patches):\n                patch_data = patches[batch_idx, patch_idx]  # (patch_size, input_dim)\n                \n                # Calculate motion directions within patch\n                if patch_size > 1:\n                    patch_directions = self.motion_processor.calculate_motion_directions(\n                        patch_data[None, :, :]  # Add batch dimension\n                    )[0]  # Remove batch dimension\n                    \n                    # Use the most common direction in the patch\n                    if len(patch_directions) > 0:\n                        direction_label = torch.mode(torch.tensor(patch_directions))[0].item()\n                    else:\n                        direction_label = -1  # Invalid\n                else:\n                    direction_label = -1  # Invalid for single point\n                \n                batch_labels.append(direction_label)\n            motion_labels.append(batch_labels)\n        \n        motion_labels_tensor = torch.LongTensor(motion_labels).to(self.device)\n        \n        # Create mask\n        mask = self.masking_strategy.create_mask(patches_tensor, behavior_indices).to(self.device)\n        \n        # Forward pass\n        self.optimizer.zero_grad()\n        outputs = self.model(patches_tensor, mask)\n        \n        # Calculate loss\n        loss_dict = self.criterion(\n            outputs['reconstructions'],\n            patches_tensor,\n            outputs['motion_logits'],\n            motion_labels_tensor,\n            mask\n        )\n        \n        # Backward pass\n        loss_dict['total_loss'].backward()\n        self.optimizer.step()\n        \n        return loss_dict\n    \n    def get_representations(self, patches):\n        \"\"\"\n        Extract representations for downstream tasks\n        Args:\n            patches: numpy array of patches (batch_size, num_patches, patch_size, input_dim)\n        Returns:\n            Global representations (batch_size, dim)\n        \"\"\"\n        self.model.eval()\n        \n        with torch.no_grad():\n            patches_tensor = torch.FloatTensor(patches).to(self.device)\n            outputs = self.model(patches_tensor)\n            return outputs['global_representation'].cpu().numpy()\n\n# class FLIGHT2VECTrainer:\n#     \"\"\"Improved training wrapper with better data handling and monitoring\"\"\"\n    \n#     def __init__(self, model, optimizer, criterion, device='cuda', masking_strategy=None):\n#         self.model = model\n#         self.optimizer = optimizer\n#         self.criterion = criterion\n#         self.device = device\n#         self.masking_strategy = masking_strategy or ImprovedMaskingStrategy()\n#         self.motion_processor = MotionDirectionProcessor()\n        \n#         # Training monitoring\n#         self.epoch = 0\n#         self.losses_history = []\n        \n#         # Add gradient clipping\n#         self.max_grad_norm = 1.0\n        \n#     def _prepare_motion_labels(patches, original_trajectories=None):\n#         batch_size, num_patches, patch_size, input_dim = patches.shape\n#         motion_labels = torch.full((batch_size, num_patches), -1, dtype=torch.long)\n    \n#         if original_trajectories is not None:\n#             try:\n#                 trajectory_motion_labels = self.motion_processor.calculate_motion_directions(original_trajectories)\n    \n#                 for batch_idx in range(batch_size):\n#                     patch_starts = [int(p[1]) for p in original_trajectories[batch_idx][\"patch_indices\"]]\n#                     patch_size = original_trajectories[batch_idx][\"patch_size\"]\n#                     labels = trajectory_motion_labels[batch_idx]\n    \n#                     for patch_idx, start_idx in enumerate(patch_starts):\n#                         end_idx = min(start_idx + patch_size - 1, len(labels))\n#                         patch_labels = labels[start_idx:end_idx]\n#                         valid_labels = patch_labels[patch_labels != -1]\n    \n#                         if len(valid_labels) > 0:\n#                             unique_labels, counts = np.unique(valid_labels, return_counts=True)\n#                             most_common = unique_labels[np.argmax(counts)]\n#                             motion_labels[batch_idx, patch_idx] = most_common\n#             except Exception as e:\n#                 print(f\"Warning: Could not compute motion labels: {e}\")\n    \n#         return motion_labels\n    \n#     def train_step(self, patches, original_trajectories=None, behavior_indices=None):\n#         \"\"\"\n#         Improved training step with better error handling\n#         \"\"\"\n#         self.model.train()\n        \n#         # Validate input shapes\n#         if not isinstance(patches, torch.Tensor):\n#             patches = torch.FloatTensor(patches)\n#         patches = patches.to(self.device)\n        \n#         batch_size, num_patches, patch_size, input_dim = patches.shape\n        \n#         # Prepare motion labels\n#         motion_labels = self._prepare_motion_labels(patches, original_trajectories)\n#         motion_labels = motion_labels.to(self.device)\n        \n#         # Create mask\n#         mask = self.masking_strategy.create_mask(patches, behavior_indices).to(self.device)\n        \n#         # Ensure at least some patches are masked\n#         if mask.sum() == 0:\n#             # Force mask at least one patch per batch\n#             for batch_idx in range(batch_size):\n#                 random_patch = torch.randint(0, num_patches, (1,)).item()\n#                 mask[batch_idx, random_patch] = True\n        \n#         # Forward pass\n#         self.optimizer.zero_grad()\n        \n#         try:\n#             outputs = self.model(patches, mask)\n            \n#             # Set current epoch for loss function\n#             if hasattr(self.criterion, 'set_epoch'):\n#                 self.criterion.set_epoch(self.epoch)\n            \n#             # Calculate loss\n#             loss_dict = self.criterion(\n#                 outputs['reconstructions'],\n#                 patches,\n#                 outputs['motion_logits'],\n#                 motion_labels,\n#                 mask\n#             )\n            \n#             # Backward pass with gradient clipping\n#             loss_dict['total_loss'].backward()\n            \n#             # Gradient clipping\n#             if self.max_grad_norm > 0:\n#                 grad_norm = torch.nn.utils.clip_grad_norm_(\n#                     self.model.parameters(), \n#                     self.max_grad_norm\n#                 )\n#                 loss_dict['grad_norm'] = grad_norm\n            \n#             self.optimizer.step()\n            \n#             # Add monitoring info\n#             loss_dict['masked_patches_ratio'] = mask.float().mean().item()\n#             loss_dict['batch_size'] = batch_size\n            \n#             return loss_dict\n            \n#         except Exception as e:\n#             print(f\"Error in training step: {e}\")\n#             # Return dummy loss dict to prevent training crash\n#             return {\n#                 'total_loss': torch.tensor(float('inf')),\n#                 'mse_loss': torch.tensor(float('inf')),\n#                 'direction_loss': torch.tensor(float('inf')),\n#                 'error': str(e)\n#             }\n    \n#         def validate_step(self, patches, original_trajectories=None, behavior_indices=None):\n#             \"\"\"Validation step without gradients\"\"\"\n#             self.model.eval()\n            \n#             with torch.no_grad():\n#                 if not isinstance(patches, torch.Tensor):\n#                     patches = torch.FloatTensor(patches)\n#                 patches = patches.to(self.device)\n                \n#                 motion_labels = self._prepare_motion_labels(patches, original_trajectories)\n#                 motion_labels = motion_labels.to(self.device)\n                \n#                 mask = self.masking_strategy.create_mask(patches, behavior_indices).to(self.device)\n                \n#                 outputs = self.model(patches, mask)\n                \n#                 if hasattr(self.criterion, 'set_epoch'):\n#                     self.criterion.set_epoch(self.epoch)\n                \n#                 loss_dict = self.criterion(\n#                     outputs['reconstructions'],\n#                     patches,\n#                     outputs['motion_logits'],\n#                     motion_labels,\n#                     mask\n#                 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:23:36.738322Z","iopub.execute_input":"2025-06-18T14:23:36.738775Z","iopub.status.idle":"2025-06-18T14:23:36.754982Z","shell.execute_reply.started":"2025-06-18T14:23:36.738749Z","shell.execute_reply":"2025-06-18T14:23:36.753786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Flight2VecLoss(nn.Module):\n    \"\"\"Combined loss function for FLIGHT2VEC\"\"\"\n    \n    def __init__(self, lambda_mse=1.0, lambda_direction=1.0):\n        super().__init__()\n        self.mse_loss = nn.MSELoss()\n        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-1)\n        self.lambda_mse = lambda_mse\n        self.lambda_direction = lambda_direction\n    \n    def forward(self, \n                reconstructions, \n                original_patches, \n                motion_logits, \n                motion_labels, \n                mask):\n        \"\"\"\n        Args:\n            reconstructions: Reconstructed patches (batch_size, num_patches, patch_size, input_dim)\n            original_patches: Original patches (batch_size, num_patches, patch_size, input_dim)\n            motion_logits: Motion direction logits (batch_size, num_patches, 26)\n            motion_labels: Motion direction labels (batch_size, num_patches)\n            mask: Boolean mask for masked patches (batch_size, num_patches)\n        \"\"\"\n        # MSE loss only on masked patches\n        masked_reconstructions = reconstructions[mask]\n        masked_originals = original_patches[mask]\n        mse_loss = self.mse_loss(masked_reconstructions, masked_originals)\n        \n        # Motion direction loss on all patches\n        batch_size, num_patches = motion_logits.shape[:2]\n        motion_logits_flat = motion_logits.view(-1, 26)\n        motion_labels_flat = motion_labels.view(-1)\n        \n        # Filter out invalid labels\n        valid_mask = motion_labels_flat != -1\n        if valid_mask.sum() > 0:\n            direction_loss = self.ce_loss(\n                motion_logits_flat[valid_mask], \n                motion_labels_flat[valid_mask]\n            )\n        else:\n            direction_loss = torch.tensor(0.0, device=motion_logits.device)\n        \n        # Combined loss\n        total_loss = (self.lambda_direction * direction_loss + \n                     self.lambda_mse * mse_loss)\n        \n        return {\n            'total_loss': total_loss,\n            'mse_loss': mse_loss,\n            'direction_loss': direction_loss\n        }\n\n# class Flight2VecLoss(nn.Module):\n#     \"\"\"Improved loss function with better balancing and monitoring\"\"\"\n    \n#     def __init__(self, lambda_mse=1.0, lambda_direction=0.1, adaptive_weights=True, warmup_epochs=10):\n#         super().__init__()\n#         self.mse_loss = nn.MSELoss()\n#         self.ce_loss = nn.CrossEntropyLoss(ignore_index=-1)\n#         self.lambda_mse = lambda_mse\n#         self.lambda_direction = lambda_direction\n#         self.adaptive_weights = adaptive_weights\n#         self.warmup_epochs = warmup_epochs\n#         self.current_epoch = 0\n        \n#     def set_epoch(self, epoch):\n#         self.current_epoch = epoch\n    \n#     def forward(self, reconstructions, original_patches, motion_logits, motion_labels, mask):\n#         batch_size = reconstructions.shape[0]\n        \n#         # MSE loss only on masked patches\n#         if mask.sum() > 0:\n#             masked_reconstructions = reconstructions[mask]\n#             masked_originals = original_patches[mask]\n#             mse_loss = self.mse_loss(masked_reconstructions, masked_originals)\n#         else:\n#             mse_loss = torch.tensor(0.0, device=reconstructions.device, requires_grad=True)\n        \n#         # Motion direction loss\n#         motion_logits_flat = motion_logits.view(-1, motion_logits.size(-1))\n#         motion_labels_flat = motion_labels.view(-1)\n        \n#         valid_mask = motion_labels_flat != -1\n#         if valid_mask.sum() > 0:\n#             direction_loss = self.ce_loss(\n#                 motion_logits_flat[valid_mask], \n#                 motion_labels_flat[valid_mask]\n#             )\n            \n#             # Calculate accuracy for monitoring\n#             with torch.no_grad():\n#                 pred_classes = motion_logits_flat[valid_mask].argmax(dim=1)\n#                 accuracy = (pred_classes == motion_labels_flat[valid_mask]).float().mean()\n#         else:\n#             direction_loss = torch.tensor(0.0, device=motion_logits.device, requires_grad=True)\n#             accuracy = torch.tensor(0.0, device=motion_logits.device)\n        \n#         # Adaptive loss weighting with warmup\n#         warmup_factor = min(1.0, self.current_epoch / self.warmup_epochs)\n#         effective_lambda_dir = self.lambda_direction * warmup_factor\n        \n#         if self.adaptive_weights and mse_loss.item() > 0 and direction_loss.item() > 0:\n#             # Balance losses based on their magnitudes\n#             mse_magnitude = mse_loss.detach()\n#             dir_magnitude = direction_loss.detach()\n#             balance_factor = torch.clamp(mse_magnitude / (dir_magnitude + 1e-8), 0.1, 10.0)\n#             effective_lambda_dir *= balance_factor\n        \n#         total_loss = self.lambda_mse * mse_loss + effective_lambda_dir * direction_loss\n        \n#         return {\n#             'total_loss': total_loss,\n#             'mse_loss': mse_loss,\n#             'direction_loss': direction_loss,\n#             'direction_accuracy': accuracy,\n#             'effective_lambda_dir': effective_lambda_dir,\n#             'num_valid_motion_labels': valid_mask.sum().item()\n#         }\n\n\n# Example usage\ndef create_flight2vec_model():\n    \"\"\"Create and initialize FLIGHT2VEC model\"\"\"\n    model = FLIGHT2VECModel(\n        input_dim=6,\n        patch_size=32,\n        dim=256,\n        depth=3,\n        heads=16,\n        dropout=0.2,\n        total_patches=64\n    )\n    \n    # Initialize model parameters\n    for name, param in model.named_parameters():\n        if 'weight' in name and param.dim() > 1:\n            nn.init.xavier_uniform_(param)\n        elif 'bias' in name:\n            nn.init.zeros_(param)\n    \n    return model\n\ndef create_training_setup():\n    \"\"\"Setup training components\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Model\n    model = create_flight2vec_model().to(device)\n    \n    # Optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=1e-5,\n        weight_decay=1e-4\n    )\n    \n    # Loss function\n    criterion = Flight2VecLoss(lambda_mse=1.0, lambda_direction=1.0)\n    \n    # Trainer\n    trainer = FLIGHT2VECTrainer(model, optimizer, criterion, device)\n    \n    return trainer\n\n# Example training loop\ndef train_flight2vec(trainer, patches_data, behavior_indices_data, epochs=100, epoch_print_interval=10):\n    \"\"\"\n    Train FLIGHT2VEC model\n    Args:\n        trainer: FLIGHT2VECTrainer instance\n        patches_data: List of patch arrays\n        behavior_indices_data: List of behavior indices for each sample\n        epochs: Number of training epochs\n    \"\"\"\n    for epoch in range(epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        # Assuming patches_data is a list of batches\n        for batch_patches, batch_behavior_indices in zip(patches_data, behavior_indices_data):\n            loss_dict = trainer.train_step(batch_patches, batch_behavior_indices)\n            total_loss += loss_dict['total_loss'].item()\n            num_batches += 1\n        \n        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n        \n        if epoch % epoch_print_interval == 0:\n            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n    \n    return trainer\n\n# def create_training_setup():\n#     \"\"\"Create improved training setup with better hyperparameters\"\"\"\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n#     # Model with improved architecture\n#     model = FLIGHT2VECModel(\n#         input_dim=6,\n#         patch_size=32,\n#         dim=512,  # Larger embedding\n#         depth=6,  # More layers\n#         heads=8,  # More attention heads\n#         dropout=0.1,  # Lower dropout\n#         total_patches=64\n#     ).to(device)\n\n#     # Initialize model parameters\n#     for name, param in model.named_parameters():\n#         if 'weight' in name and param.dim() > 1:\n#             nn.init.xavier_uniform_(param)\n#         elif 'bias' in name:\n#             nn.init.zeros_(param)\n    \n#     # Better optimizer with weight decay\n#     optimizer = torch.optim.AdamW(\n#         model.parameters(),\n#         lr=1e-4,  # Lower learning rate\n#         weight_decay=1e-3,\n#         betas=(0.9, 0.999)\n#     )\n    \n#     # Improved loss function\n#     criterion = Flight2VecLoss(\n#         lambda_mse=1.0, \n#         lambda_direction=0.1,  # Lower weight for direction loss\n#         adaptive_weights=True,\n#         warmup_epochs=10\n#     )\n    \n#     # Improved masking strategy\n#     masking_strategy = MaskingStrategy(\n#         behavior_mask_prob=0.4,  # More conservative\n#         non_behavior_mask_prob=0.2,\n#         min_unmasked=0.3\n#     )\n    \n#     # Trainer with improvements\n#     trainer = ImprovedFLIGHT2VECTrainer(\n#         model, optimizer, criterion, device, masking_strategy\n#     )\n    \n#     return trainer\n\n# def train_flight2vec(trainer, patches_data, trajectories_data=None, behavior_indices_data=None, \n#                             epochs=100, batch_size=16, validation_split=0.2):\n#     \"\"\"\n#     Improved training function with validation and monitoring\n#     \"\"\"\n#     # Split data for validation\n#     total_samples = len(patches_data)\n#     val_size = int(total_samples * validation_split)\n#     train_size = total_samples - val_size\n    \n#     # Simple split (in practice, you might want stratified split)\n#     train_patches = patches_data[:train_size]\n#     val_patches = patches_data[train_size:]\n    \n#     train_trajectories = trajectories_data[:train_size] if trajectories_data else None\n#     val_trajectories = trajectories_data[train_size:] if trajectories_data else None\n    \n#     train_behavior = behavior_indices_data[:train_size] if behavior_indices_data else None\n#     val_behavior = behavior_indices_data[train_size:] if behavior_indices_data else None\n    \n#     print(f\"Training samples: {train_size}, Validation samples: {val_size}\")\n    \n#     # Training loop with monitoring\n#     best_val_loss = float('inf')\n#     patience = 10\n#     patience_counter = 0\n    \n#     for epoch in range(epochs):\n#         trainer.epoch = epoch\n        \n#         # Training phase\n#         trainer.model.train()\n#         train_losses = []\n        \n#         # Create batches for training\n#         for i in range(0, len(train_patches), batch_size):\n#             batch_patches = train_patches[i:i+batch_size]\n#             batch_trajectories = train_trajectories[i:i+batch_size] if train_trajectories else None\n#             batch_behavior = train_behavior[i:i+batch_size] if train_behavior else None\n            \n#             # Convert to tensor if needed\n#             if isinstance(batch_patches, list):\n#                 batch_patches = torch.stack([torch.tensor(p, dtype=torch.float32) for p in batch_patches])\n            \n#             loss_dict = trainer.train_step(batch_patches, batch_trajectories, batch_behavior)\n            \n#             if 'error' not in loss_dict:\n#                 train_losses.append({k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()})\n        \n#         # Validation phase\n#         if val_patches:\n#             trainer.model.eval()\n#             val_losses = []\n            \n#             for i in range(0, len(val_patches), batch_size):\n#                 batch_patches = val_patches[i:i+batch_size]\n#                 batch_trajectories = val_trajectories[i:i+batch_size] if val_trajectories else None\n#                 batch_behavior = val_behavior[i:i+batch_size] if val_behavior else None\n                \n#                 if isinstance(batch_patches, list):\n#                     batch_patches = torch.stack([torch.tensor(p, dtype=torch.float32) for p in batch_patches])\n                \n#                 loss_dict = trainer.validate_step(batch_patches, batch_trajectories, batch_behavior)\n#                 val_losses.append({k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()})\n        \n#         # Calculate average losses\n#         if train_losses:\n#             avg_train_loss = np.mean([l['total_loss'] for l in train_losses if l['total_loss'] != float('inf')])\n#             avg_train_mse = np.mean([l['mse_loss'] for l in train_losses if l['mse_loss'] != float('inf')])\n#             avg_train_dir = np.mean([l['direction_loss'] for l in train_losses if l['direction_loss'] != float('inf')])\n#         else:\n#             avg_train_loss = avg_train_mse = avg_train_dir = float('inf')\n        \n#         if val_losses:\n#             avg_val_loss = np.mean([l['total_loss'] for l in val_losses if l['total_loss'] != float('inf')])\n#             avg_val_mse = np.mean([l['mse_loss'] for l in val_losses if l['mse_loss'] != float('inf')])\n#             avg_val_dir = np.mean([l['direction_loss'] for l in val_losses if l['direction_loss'] != float('inf')])\n#         else:\n#             avg_val_loss = avg_val_mse = avg_val_dir = float('inf')\n        \n#         # Print progress\n#         if epoch % 5 == 0:\n#             print(f\"Epoch {epoch}/{epochs}\")\n#             print(f\"  Train - Total: {avg_train_loss:.4f}, MSE: {avg_train_mse:.4f}, Dir: {avg_train_dir:.4f}\")\n#             if val_losses:\n#                 print(f\"  Val   - Total: {avg_val_loss:.4f}, MSE: {avg_val_mse:.4f}, Dir: {avg_val_dir:.4f}\")\n            \n#             # Print additional monitoring info\n#             if train_losses:\n#                 sample_loss = train_losses[-1]\n#                 if 'direction_accuracy' in sample_loss:\n#                     print(f\"  Direction Accuracy: {sample_loss['direction_accuracy']:.3f}\")\n#                 if 'masked_patches_ratio' in sample_loss:\n#                     print(f\"  Masked Patches Ratio: {sample_loss['masked_patches_ratio']:.3f}\")\n#                 if 'num_valid_motion_labels' in sample_loss:\n#                     print(f\"  Valid Motion Labels: {sample_loss['num_valid_motion_labels']}\")\n        \n#         # Early stopping based on validation loss\n#         if val_losses and avg_val_loss < best_val_loss:\n#             best_val_loss = avg_val_loss\n#             patience_counter = 0\n#             # Save best model\n#             torch.save(trainer.model.state_dict(), 'best_flight2vec_model.pth')\n#         else:\n#             patience_counter += 1\n            \n#         if patience_counter >= patience:\n#             print(f\"Early stopping at epoch {epoch} (patience={patience})\")\n#             break\n    \n#     return trainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:23:42.469535Z","iopub.execute_input":"2025-06-18T14:23:42.470290Z","iopub.status.idle":"2025-06-18T14:23:42.489831Z","shell.execute_reply.started":"2025-06-18T14:23:42.470253Z","shell.execute_reply":"2025-06-18T14:23:42.488654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Additional debugging utilities\nclass Flight2VecDebugger:\n    \"\"\"Utility class for debugging FLIGHT2VEC training\"\"\"\n    \n    def __init__(self, model, device='cuda'):\n        self.model = model\n        self.device = device\n    \n    def check_gradients(self):\n        \"\"\"Check if gradients are flowing properly\"\"\"\n        total_norm = 0\n        param_count = 0\n        zero_grad_count = 0\n        \n        for name, param in self.model.named_parameters():\n            if param.grad is not None:\n                param_norm = param.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n                param_count += 1\n                \n                if param_norm.item() < 1e-8:\n                    zero_grad_count += 1\n                    print(f\"Very small gradient in {name}: {param_norm.item()}\")\n            else:\n                print(f\"No gradient for parameter: {name}\")\n        \n        total_norm = total_norm ** (1. / 2)\n        print(f\"Total gradient norm: {total_norm}\")\n        print(f\"Parameters with gradients: {param_count}\")\n        print(f\"Parameters with near-zero gradients: {zero_grad_count}\")\n        \n        return total_norm\n    \n    def analyze_data_batch(self, patches, motion_labels=None):\n        \"\"\"Analyze a batch of data for potential issues\"\"\"\n        print(f\"Batch shape: {patches.shape}\")\n        print(f\"Data range: [{patches.min().item():.6f}, {patches.max().item():.6f}]\")\n        print(f\"Data mean: {patches.mean().item():.6f}, std: {patches.std().item():.6f}\")\n        \n        # Check for NaN or inf values\n        nan_count = torch.isnan(patches).sum().item()\n        inf_count = torch.isinf(patches).sum().item()\n        if nan_count > 0 or inf_count > 0:\n            print(f\"WARNING: Found {nan_count} NaN and {inf_count} inf values\")\n        \n        # Check motion labels if provided\n        if motion_labels is not None:\n            unique_labels = torch.unique(motion_labels[motion_labels != -1])\n            print(f\"Unique motion labels: {unique_labels.tolist()}\")\n            print(f\"Motion label distribution:\")\n            for label in unique_labels:\n                count = (motion_labels == label).sum().item()\n                print(f\"  Label {label}: {count} occurrences\")\n    \n    def test_forward_pass(self, patches):\n        \"\"\"Test forward pass and check outputs\"\"\"\n        self.model.eval()\n        \n        with torch.no_grad():\n            try:\n                outputs = self.model(patches)\n                \n                print(\"Forward pass successful!\")\n                print(f\"Representations shape: {outputs['representations'].shape}\")\n                print(f\"Global representation shape: {outputs['global_representation'].shape}\")\n                print(f\"Reconstructions shape: {outputs['reconstructions'].shape}\")\n                print(f\"Motion logits shape: {outputs['motion_logits'].shape}\")\n                \n                # Check output ranges\n                for key, tensor in outputs.items():\n                    if torch.is_tensor(tensor):\n                        print(f\"{key} - range: [{tensor.min().item():.6f}, {tensor.max().item():.6f}]\")\n                        nan_count = torch.isnan(tensor).sum().item()\n                        if nan_count > 0:\n                            print(f\"  WARNING: {nan_count} NaN values in {key}\")\n                \n            except Exception as e:\n                print(f\"Forward pass failed: {e}\")\n                import traceback\n                traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:40.363165Z","iopub.execute_input":"2025-06-18T14:02:40.363508Z","iopub.status.idle":"2025-06-18T14:02:40.376501Z","shell.execute_reply.started":"2025-06-18T14:02:40.363481Z","shell.execute_reply":"2025-06-18T14:02:40.375455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_batches(patches_data: List, behavior_indices_data: List, batch_size: int) -> Tuple[List, List]:\n    \"\"\"\n    Create batches from patches and behavior indices data\n    Args:\n        patches_data: List of patch arrays (each array shape: [64, 32, 6])\n        behavior_indices_data: List of behavior indices (each element is a list of variable length)\n        batch_size: Size of each batch\n    Returns:\n        Tuple of (batched_patches, batched_behavior_indices)\n    \"\"\"\n    batched_patches = []\n    batched_behavior_indices = []\n    \n    for i in range(0, len(patches_data), batch_size):\n        batch_patches = patches_data[i:i + batch_size]\n        batch_behavior_indices = behavior_indices_data[i:i + batch_size]\n        \n        # Handle patches - these should be stackable since they have the same shape\n        if isinstance(batch_patches[0], np.ndarray):\n            # Stack all patches in the batch into a single tensor\n            # Shape: [batch_size, 64, 32, 6]\n            batch_patches_tensor = torch.stack([\n                torch.tensor(patch, dtype=torch.float32) for patch in batch_patches\n            ])\n        else:\n            # Assume they're already tensors\n            batch_patches_tensor = torch.stack(batch_patches)\n        \n        # Handle behavior indices - these have variable lengths, so keep as list of tensors\n        batch_behavior_list = []\n        for behavior_idx_list in batch_behavior_indices:\n            if isinstance(behavior_idx_list, list):\n                # Convert each list to tensor\n                behavior_tensor = torch.tensor(behavior_idx_list, dtype=torch.long)\n            else:\n                # Already a tensor or array\n                behavior_tensor = torch.tensor(behavior_idx_list, dtype=torch.long)\n            batch_behavior_list.append(behavior_tensor)\n        \n        batched_patches.append(batch_patches_tensor)\n        batched_behavior_indices.append(batch_behavior_list)\n    \n    return np.array(batched_patches), batched_behavior_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:35:11.167810Z","iopub.execute_input":"2025-06-18T15:35:11.168857Z","iopub.status.idle":"2025-06-18T15:35:11.181925Z","shell.execute_reply.started":"2025-06-18T15:35:11.168811Z","shell.execute_reply":"2025-06-18T15:35:11.180755Z"}},"outputs":[],"execution_count":169},{"cell_type":"code","source":"def run_debugging_session():\n    \"\"\"Run a complete debugging session\"\"\"\n    print(\"FLIGHT2VEC Debugging Session\")\n    print(\"=\" * 50)\n    \n    # Create model and trainer\n    print(\"\\nSetting up model and trainer...\")\n    trainer = create_training_setup()\n    debugger = Flight2VecDebugger(trainer.model, trainer.device)\n    \n    # Test data analysis\n    print(\"\\nAnalyzing sample batch...\")\n    sample_patches = torch.tensor(patches_raw[0:2], dtype=torch.float32).to(trainer.device)\n    debugger.analyze_data_batch(sample_patches)\n    \n    # Test forward pass\n    print(\"\\nTesting forward pass...\")\n    debugger.test_forward_pass(sample_patches)\n    \n    # Test training step\n    print(\"\\nTesting training step...\")\n    try:\n        loss_dict = trainer.train_step(\n            patches_raw[0:2],\n            patches_behavior_idxs[0:2]\n        )\n        \n        print(\"Training step successful!\")\n        for key, value in loss_dict.items():\n            if torch.is_tensor(value):\n                print(f\"{key}: {value.item():.6f}\")\n            else:\n                print(f\"{key}: {value}\")\n        \n        # Check gradients\n        print(\"\\nChecking gradients...\")\n        debugger.check_gradients()\n            \n    except Exception as e:\n        print(f\"Training step failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n    # Test a few training iterations\n    print(\"\\nTesting short training run...\")\n    batch_patches, batch_behavior_idxs = create_batches(patches_raw[:2], patches_behavior_idxs[:2], batch_size=1)\n    try:\n        trainer_result = train_flight2vec(\n            trainer=trainer,\n            patches_data=batch_patches,\n            behavior_indices_data=batch_behavior_idxs,\n            epochs=1000,\n            epoch_print_interval=1,\n            # batch_size=2,\n            # validation_split=0.3\n        )\n        print(\"Short training run completed successfully!\")\n        \n    except Exception as e:\n        print(f\"Training run failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    run_debugging_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:27:23.132684Z","iopub.execute_input":"2025-06-18T14:27:23.133049Z","iopub.status.idle":"2025-06-18T14:27:33.516421Z","shell.execute_reply.started":"2025-06-18T14:27:23.133025Z","shell.execute_reply":"2025-06-18T14:27:33.514903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train_flight2vec(trainer, patches_data, behavior_indices_data, epochs=100, batch_size=32):\n#     \"\"\"\n#     Train FLIGHT2VEC model\n#     Args:\n#         trainer: FLIGHT2VECTrainer instance\n#         patches_data: List of patch arrays\n#         behavior_indices_data: List of behavior indices for each sample\n#         epochs: Number of training epochs\n#         batch_size: Batch size for training\n#     \"\"\"\n#     # Create batches\n#     print(f\"Creating batches with batch_size={batch_size}\")\n#     batched_patches, batched_behavior_indices = create_batches(\n#         patches_data, behavior_indices_data, batch_size\n#     )\n    \n#     print(f\"Total samples: {len(patches_data)}\")\n#     print(f\"Number of batches: {len(batched_patches)}\")\n    \n#     for epoch in range(epochs):\n#         total_loss = 0\n#         num_batches = 0\n        \n#         # Iterate through batches\n#         for batch_idx, (batch_patches, batch_behavior_indices) in enumerate(\n#             zip(batched_patches, batched_behavior_indices)\n#         ):\n#             try:\n#                 loss_dict = trainer.train_step(batch_patches, batch_behavior_indices)\n                \n#                 # Handle loss extraction more robustly\n#                 if isinstance(loss_dict['total_loss'], torch.Tensor):\n#                     if loss_dict['total_loss'].numel() == 1:\n#                         batch_loss = loss_dict['total_loss'].item()\n#                     else:\n#                         # If tensor has multiple elements, take the mean\n#                         batch_loss = loss_dict['total_loss'].mean().item()\n#                 else:\n#                     # If it's already a scalar\n#                     batch_loss = float(loss_dict['total_loss'])\n                \n#                 total_loss += batch_loss\n#                 num_batches += 1\n                \n#                 # Print batch progress occasionally\n#                 if batch_idx % 20 == 0 and epoch % 10 == 0:\n#                     print(f\"  Batch {batch_idx}/{len(batched_patches)}, Loss: {batch_loss:.4f}\")\n                    \n#             except Exception as e:\n#                 print(f\"Error in batch {batch_idx}: {e}\")\n#                 import traceback\n#                 traceback.print_exc()\n#                 continue\n        \n#         avg_loss = total_loss / num_batches if num_batches > 0 else 0\n        \n#         if epoch % 10 == 0:\n#             print(f\"Epoch {epoch}/{epochs}, Average Loss: {avg_loss:.4f}\")\n    \n#     return trainer\n\n# def validate_data(patches_raw, patches_behavior_idxs):\n#     \"\"\"\n#     Validate input data before training\n#     \"\"\"\n#     if len(patches_raw) != len(patches_behavior_idxs):\n#         raise ValueError(f\"Mismatch in data lengths: patches={len(patches_raw)}, behavior_indices={len(patches_behavior_idxs)}\")\n    \n#     if len(patches_raw) == 0:\n#         raise ValueError(\"No training data provided\")\n    \n#     print(f\"Data validation passed: {len(patches_raw)} samples\")\n    \n#     # Print sample shapes\n#     if hasattr(patches_raw[0], 'shape'):\n#         print(f\"Sample patch shape: {patches_raw[0].shape}\")\n#     if hasattr(patches_behavior_idxs[0], '__len__'):\n#         print(f\"Sample behavior indices length: {len(patches_behavior_idxs[0])}\")\n        \n#     # Show distribution of behavior indices lengths\n#     behavior_lengths = [len(idx_list) for idx_list in patches_behavior_idxs]\n#     print(f\"Behavior indices lengths - Min: {min(behavior_lengths)}, Max: {max(behavior_lengths)}, Avg: {np.mean(behavior_lengths):.2f}\")\n    \n#     # Show a few examples\n#     print(f\"First few behavior indices: {patches_behavior_idxs[:3]}\")\n\n# Main training function\ndef main():\n    \"\"\"\n    Main training function with proper data handling and batching\n    \"\"\"\n    print(\"Starting FLIGHT2VEC Training\")\n    print(\"=\" * 50)\n    \n    # Training parameters\n    BATCH_SIZE = 16  # Adjust based on your GPU memory\n    EPOCHS = 100\n    LEARNING_RATE = 0.001\n    \n    try:\n        # Setup training (assuming this function exists)\n        print(\"Setting up trainer...\")\n        trainer = setup_training()  # You'll need to implement this\n\n        patches_raw = np.array([np.array(p[0]) for p in patches.values()])\n        patches_behavior_idxs = [p[1] for p in patches.values()]\n        \n        # Validate data\n        validate_data(patches_raw, patches_behavior_idxs)\n        \n        # Train the model\n        print(f\"\\nStarting training with:\")\n        print(f\"  Batch size: {BATCH_SIZE}\")\n        print(f\"  Epochs: {EPOCHS}\")\n        print(f\"  Samples: {len(patches_raw)}\")\n        print(\"-\" * 30)\n        \n        trained_trainer = train_flight2vec(\n            trainer=trainer,\n            patches_data=patches_raw,\n            behavior_indices_data=patches_behavior_idxs,\n            epochs=EPOCHS,\n            batch_size=BATCH_SIZE\n        )\n        \n        print(\"\\nTraining completed successfully!\")\n        \n        # Save the trained model (optional)\n        torch.save(trained_trainer.model.state_dict(), 'flight2vec_model.pth')\n        print(\"Model saved to 'flight2vec_model.pth'\")\n        \n    except Exception as e:\n        print(f\"Training failed with error: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:02:41.877126Z","iopub.execute_input":"2025-06-18T14:02:41.877463Z","iopub.status.idle":"2025-06-18T14:02:41.930390Z","shell.execute_reply.started":"2025-06-18T14:02:41.877427Z","shell.execute_reply":"2025-06-18T14:02:41.928934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}